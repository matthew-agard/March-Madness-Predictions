{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Python libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Global variables\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime\n",
    "start_year = 1993\n",
    "curr_year = datetime.now().year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Custom API\n",
    "from sys import path\n",
    "\n",
    "path.append('../API/eda')\n",
    "from data_visualizations import get_yearly_base_rates, get_seed_pairs, format_plot\n",
    "\n",
    "path.append('../API/fetch')\n",
    "import data_fetch as fetch\n",
    "\n",
    "path.append('../API/model')\n",
    "from model_selection import get_cv_models\n",
    "from model_evaluation import evaluate_cv_models, model_predictions, get_classification_report\n",
    "\n",
    "path.append('../API/preprocess')\n",
    "from feature_engineering import create_bracket_winners\n",
    "from data_integrity import season_team_to_coach_team_dict, curr_season_to_tourney_dict\n",
    "from data_clean import clean_basic_stats, clean_tourney_data\n",
    "from data_pipeline import dataset_pipeline, feature_pipeline, bracket_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Fetching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceived Predictors\n",
    "\n",
    "Naturally, it will be vitally important to scrape available data that is pertinent to deciding the outcome of an NCAA March Madness game between any two given teams. To successfully do so, we must break down what are generally the most influential elements of a basketball team's success.\n",
    "\n",
    "<br>Overall team performance during the regular season is generally a good indicator of how a team will perform in March Madness. This would be captured by statistics, both basic and advanced, such as the following:\n",
    "<br>**Season Record (%)\n",
    "<br>Conference Record (%); could be important given that the tournament is split into regions\n",
    "<br>Regular Season Record vs. Tourney Opponent (%); set to theoretical discrete probability of 50% if no such matchups exist \n",
    "<br>Strength of Schedule (SOS); measures the difficulty of the teams played (higher number = greater difficulty)\n",
    "<br>Top 25 Ranking (boolean); considered a consensus top-tier team\n",
    "<br>Shots Made per Game (FG, 3P, FT)\n",
    "<br>Point Differential per Game; measures how dominant/unsuccessful you are at outscoring your opponent on average\n",
    "<br>Misc. Team Stats per Game (Rebounds, Assists, Blocks, etc.)**\n",
    "\n",
    "<br>It's important to note that in the NCAA, more so than the NBA, experienced coaches can have just as much of an impact on a game's outcome as the players themselves. Hence, it's reasonable to assume that the following statistics could also be solid indicators:\n",
    "**<br>Coach March Madness Appearances\n",
    "<br>Coach Sweet Sixteen Appearances\n",
    "<br>Coach Final Four Appearances\n",
    "<br>Coach Championships Won**\n",
    "\n",
    "<br>And lastly, we need the data for the structure of the tournaments themselves:\n",
    "**<br>Favorite Seed\n",
    "<br>Underdog Seed\n",
    "<br>Round Number (0-6)\n",
    "<br>Game Outcome (boolean); did the underdog upset the favorite?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline\n",
    "\n",
    "Below is the output of a merge of the datasets displayed above (after they've been cleaned). Once we remove the features with nulls that won't be imputed, we can begin our exploratory data analysis (EDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous years permitted the provision of examples as to how the predictors listed above were scraped. However, due to request rate limit restrictions from SportsReference, the inclusion of these examples in the research notebook has been discontinued. Please refer to the Python API for examples moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tournament Game Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hist_bracket_df = fetch.get_hist_bracket(curr_year-1)\n",
    "hist_bracket_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check if the historical data CSV exists, if it doesn't then create it\n",
    "try:\n",
    "    mm_matchups_df = pd.read_csv(f'{curr_year}_march_madness_hist_data.csv')\n",
    "except FileNotFoundError:\n",
    "    hist_matchups_df = pd.read_csv(f'{curr_year-1}_march_madness_hist_data.csv')\n",
    "    curr_matchups_df = dataset_pipeline(np.arange(curr_year, curr_year+1))\n",
    "    \n",
    "    mm_matchups_df = pd.concat([hist_matchups_df, curr_matchups_df])\n",
    "    mm_matchups_df.to_csv(f'{curr_year}_march_madness_hist_data.csv', index=False)\n",
    "\n",
    "mm_matchups_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Nulls\n",
    "\n",
    "Recall that our cleaned dataset has a total of 1,814 March Madness games present, and each March Madness will consist of no more than 67 games (4 play-in games + 63 tournament games). As can be seen below, the advanced stats pertaining to personal fouls (PF) and turnovers (TOV) are the only features listed with no more than a season's worth of missing data. Given how rapidly the gameplay of college basketball has evolved over the time horizon of our dataset, imputing nulls on multiple years of March Madness data would likely be a futile effort. The features with a high volume of nulls (1> season) will be dropped from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Given that a feature has any nulls, find the number of nulls present\n",
    "true_nulls = fetch.get_feature_null_counts(mm_matchups_df)\n",
    "true_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get turnover features from true_nulls\n",
    "tov_null_fills = [col for col in true_nulls.index if ('TOV' in col)]\n",
    "\n",
    "# Get personal foul features from true_nulls\n",
    "pf_null_fills = [col for col in true_nulls.index if ('PF' in col)]\n",
    "\n",
    "# All other features found in true_nulls are dropped from our original dataset\n",
    "null_drops = list(set(true_nulls.index) - set(tov_null_fills) - set(pf_null_fills))\n",
    "mm_matchups_df.drop(null_drops, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the rows containing all of the dataset's nulls to be imputed\n",
    "# This will be a useful reference to validate the proper imputation of the nulls\n",
    "tov_nulls_rows = fetch.get_null_rows(tov_null_fills, mm_matchups_df)\n",
    "pf_nulls_rows = fetch.get_null_rows(pf_null_fills, mm_matchups_df)\n",
    "\n",
    "display(tov_nulls_rows), display(pf_nulls_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the Distributions of the Features We Wish to Impute?\n",
    "\n",
    "Upon looking at the distributions below of all turnover and personal foul features, we can see that they closely resemble a normal distribution. Given this assumption, the features' means will serve as good values for imputing the nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the distributions of the dataset's turnover features\n",
    "tov_null_years = sorted(list(set(tov_nulls_rows['Year'])))\n",
    "\n",
    "for year in tov_null_years:\n",
    "    print(f\"{year} feature distributions\")\n",
    "    year_df = mm_matchups_df[mm_matchups_df['Year'] == year]\n",
    "    year_df[tov_null_fills].hist(figsize=(10, 5), layout=(len(tov_null_years), len(tov_null_fills)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the distributions of the dataset's personal foul features\n",
    "pf_null_years = sorted(list(set(pf_nulls_rows['Year'])))\n",
    "\n",
    "mm_matchups_df[pf_null_fills].hist(figsize=(10, 5), layout=(1, len(pf_null_fills)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute TOV Nulls by Season & Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Retrieve TOV feature means by season; only include seasons that contain nulls\n",
    "tov_col_means = mm_matchups_df[tov_nulls_rows.columns].groupby(['Year', 'Underdog_Upset']).mean()\n",
    "tov_col_means.loc[tov_null_years]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in tov_null_years:\n",
    "    for label in [0, 1]:\n",
    "        for col in tov_null_fills:\n",
    "            # Get feature's rows with nulls for given year & label type\n",
    "            fill_condition = (tov_nulls_rows['Year'] == year) & (tov_nulls_rows['Underdog_Upset'] == label)\n",
    "            col_fill_rows = tov_nulls_rows[fill_condition].index\n",
    "            # Get feature's mean for given year\n",
    "            col_year_mean = np.round(tov_col_means.loc[(year, label), col], 1)\n",
    "            # Impute nulls of interest\n",
    "            mm_matchups_df.loc[col_fill_rows, col] = mm_matchups_df.loc[col_fill_rows, col].fillna(col_year_mean)\n",
    "\n",
    "# Display rows that originally had nulls to see if they match discovered feature means\n",
    "mm_matchups_df.loc[tov_nulls_rows.index, tov_null_fills]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute PF Nulls by Target Variable Using Entire Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve PF feature means; only found in one season\n",
    "pf_col_means = mm_matchups_df[pf_nulls_rows.columns].groupby(['Underdog_Upset']).mean()\n",
    "pf_col_means.drop('Year', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for label in [0, 1]:\n",
    "    for col in pf_null_fills:\n",
    "        # Get feature's rows with nulls for given year & label type\n",
    "        fill_condition = (pf_nulls_rows['Underdog_Upset'] == label)\n",
    "        col_fill_rows = pf_nulls_rows[fill_condition].index\n",
    "        # Get feature's mean\n",
    "        col_mean = np.round(pf_col_means.loc[label, col], 1)\n",
    "        # Impute nulls of interest\n",
    "        mm_matchups_df.loc[col_fill_rows, col] = mm_matchups_df.loc[col_fill_rows, col].fillna(col_mean)\n",
    "    \n",
    "# Display rows that originally had nulls to see if they match discovered feature means\n",
    "mm_matchups_df.loc[pf_nulls_rows.index, pf_null_fills]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "\n",
    "As any good data scientist should do, there are a few questions I hope to address in my EDA:\n",
    " - What is our bracket's accuracy if we guess the favorite always wins?\n",
    " - How often do upsets occur in a given year's March Madness?\n",
    " - What is the distribution of upsets across the tournament rounds?\n",
    " - Which seeding combinations are most likely to produce upsets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is our Bracket's Accuracy if We Guess the Favorite Always Wins?\n",
    "\n",
    "This is a strategy many of us have employed at least once while filling out a March Madness bracket, myself included. This is a classic EDA question which explores the idea of the base rate: historically, how often would we be right if we always assumed the favorite won the March Madness matchup? ~68.6% of our predictions would be correct according to our data, approximately 2 out of every 3 games. This means that for any model to be of value to us, it must demonstrate >68.7% of its predictions are correct.\n",
    "\n",
    "In college basketball, the best (and thus most impactful) players typically leave the NCAA after no more than 2 years. Thus, a 2-year moving average was chosen to illustrate the base rate's trends as new impactful players participate in March Madness. Our base rate is maintained fairly consistently over time, with a few exceptions scattered across the dataset's time horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each season's base rate\n",
    "yearly_base_rates = get_yearly_base_rates(mm_matchups_df)\n",
    "# Dataset's mean base rate\n",
    "mean_base_rate = np.round(yearly_base_rates.mean(), 3)\n",
    "# Moving average\n",
    "years_ma = 2\n",
    "base_rate_ma = np.round(yearly_base_rates.rolling(years_ma).mean(), 3)\n",
    "\n",
    "# Plot findings\n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "plt.plot(yearly_base_rates.index, [mean_base_rate] * len(yearly_base_rates), color='k', linewidth=3, label=f'Mean ({mean_base_rate})')\n",
    "plt.plot(yearly_base_rates.index, base_rate_ma, color='r', linewidth=3, label=f'{years_ma}-Yr MA')\n",
    "plt.bar(yearly_base_rates.index, yearly_base_rates)\n",
    "\n",
    "format_plot(title='Dataset Base Rate Trends', xlabel='Season', ylabel='Base Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Often Do Upsets Occur in a Given Year's March Madness?\n",
    "\n",
    "If favorites win March Madness matchups ~68.7% of the time, then we can conclude the underdogs are winning the other 31.3%; this translates to approximately 20 upsets per year. This knowledge gives us insight into how many upsets we should expect our model to predict (i.e. 25 is likely too high, 15 is likely too low).\n",
    "\n",
    "A 2-year moving average has been employed once again to highlight the trends present across the dataset's time horizon. At first it may appear to be more volatile than the base rate trends, but this is simply due to the scale of the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each season's upset count\n",
    "yearly_upsets = mm_matchups_df.groupby('Year').agg({'Underdog_Upset': 'sum'})\n",
    "# Dataset's mean upset count\n",
    "mean_upsets = np.round(yearly_upsets['Underdog_Upset'].mean(), 1)\n",
    "# Moving average\n",
    "upsets_ma = np.round(yearly_upsets.rolling(years_ma).mean(), 1)\n",
    "\n",
    "# Plot findings\n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "plt.plot(yearly_upsets.index, [mean_upsets] * len(yearly_upsets), color='k', linewidth=3, label=f'Mean ({mean_upsets})')\n",
    "plt.plot(yearly_upsets.index, upsets_ma, color='r', linewidth=3, label=f'{years_ma}-Yr MA')\n",
    "plt.bar(yearly_upsets.index, yearly_upsets['Underdog_Upset'])\n",
    "\n",
    "format_plot(title='Dataset Upsets Volume Trends', xlabel='Season', ylabel='Upsets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the Distribution of Upsets Across the Tournament Rounds?\n",
    "\n",
    "Of the 20 upsets that we can expect in any given March Madness, the visualization below gives us an idea of how many upsets we can expect in each round. Nearly 70% of all upsets happen in the first 2 rounds, which is sensible given that 75% of all games are held in the first 2 rounds. Fewer upsets in each successive round is expected not only because of a diminished volume of games, but also a greater saturation of top-tier teams amongst the remaining teams in contention. We should expect our model to follow a similar pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve data about March Madness matchup seed pairings\n",
    "seed_pairs = get_seed_pairs(mm_matchups_df)\n",
    "# Extract seed pairs that resulted in upsets\n",
    "upset_pairs = seed_pairs[seed_pairs['Underdog_Upset'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group upset seed pairs by round and count them\n",
    "upset_rounds_freq = upset_pairs['Round'].value_counts(normalize=True)\n",
    "\n",
    "# Plot findings\n",
    "upset_rounds_freq.plot(figsize=(9, 6), kind='bar', rot=15)\n",
    "\n",
    "format_plot(title='Dataset Upsets by Round', xlabel='Round', ylabel='Ratio of Upsets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which Seeding Combinations are the Most Likely to Produce Upsets?\n",
    "\n",
    "Suppose our model predicts 12 upsets in the first round, even though we should only expect 9 according to our EDA. In the event this type of situation arises, it'd be valuable to know which seed pairings are most likely to produce upsets so we could discard 3 of the 12 first round upsets with the lowest upset likelihoods.\n",
    "\n",
    "Only the top 25 upset likelihoods are shown because our EDA also revealed we should expect no more than 25 upsets in a given year. We can see once again that the overwhelming majority of upsets occur in the first round, particularly in the 4 seed pairings that are the most evenly matched (Seeds 8 vs. 9 through Seeds 5 vs. 12). Our model should emulate this behavior fairly closely in its upset predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group upsets by seed pairing and count them\n",
    "upset_pairs_freq = np.round(upset_pairs['Pairs'].value_counts(normalize=True)[:25], 3)\n",
    "\n",
    "# Plot findings\n",
    "upset_pairs_freq.plot(figsize=(9, 6), kind='bar', rot=35)\n",
    "\n",
    "format_plot(title='Dataset Upsets by Seed Pair', xlabel='Seed Pair', ylabel='Ratio of Upsets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering & Analysis\n",
    "\n",
    "Below we can see our original feature matrix (X) and then our feature matrix prepped for model fitting (prep_all_X). This transformation is performed by passing X through the feature pipeline. \n",
    "\n",
    "One of the primary actions that transpires in the pipeline is subtracting the favorites' stats from the underdogs' stats in each matchups to create underdog relative features. This not only retains virtually all of our potential information gain, but also improves computation speed and reduces the likelihood of the model overfitting the data.\n",
    "\n",
    "The second primary action is the scaling of our numerical features. This conversion of all numerical features to a normal distribution is important because it eliminates the possibility of features' number ranges negatively influencing a model's  learning process.\n",
    "\n",
    "Should you be curious about the more intricate details, you may refer to the feature_pipeline() custom API documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop unneeded features\n",
    "mm_matchups_df.drop(['Year', 'Team_Favorite', 'Team_Underdog'], axis=1, inplace=True)\n",
    "# Store rounds data (for EDA visualizations)\n",
    "all_rounds = mm_matchups_df['Round']\n",
    "\n",
    "# Create feature matrix and target variable\n",
    "X = mm_matchups_df.drop('Underdog_Upset', axis=1)\n",
    "y = mm_matchups_df['Underdog_Upset']\n",
    "\n",
    "display(X), display(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"We'll stratify the 80-20 split of our training & test datasets according to the target\n",
    "variable's distribution so our model can learn the trends observed in our EDA\"\"\"\n",
    "# Drop round data because it's already stored in previous notebook cell\n",
    "X.drop('Round', axis=1, inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, \n",
    "                                                    stratify=pd.concat([y, all_rounds], axis=1))\n",
    "\n",
    "# All datasets will be scaled based on fit found for training dataset\n",
    "basic_stats_cols = clean_basic_stats(basic_stats_df).columns\n",
    "data_cuts = {    \n",
    "    'FULL': X,\n",
    "    'TRAIN': X_train,\n",
    "    'TEST': X_test,\n",
    "}\n",
    "\n",
    "# Pass datasets through feature pipeline to prep them for model fitting\n",
    "prep_X_train = feature_pipeline('TRAIN', data_cuts, basic_stats_cols)\n",
    "prep_X_test = feature_pipeline('TEST', data_cuts, basic_stats_cols)\n",
    "\n",
    "prep_all_X = pd.concat([prep_X_train, prep_X_test])\n",
    "prep_all_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the Distributions of all our Features?\n",
    "\n",
    "The illustration below shows us the impact of the feature scaling mentioned earlier. Our features are predominantly normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_all_X.hist(figsize=(15, 10))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Features have the Greatest Predictive Power?\n",
    "The plot below represents an important concept in data science & machine learning: normalized information gain. It's a ratio of how much predictive power can be attributed to each feature in a feature matrix; these values should sum to 1, or 100% of the predictive power.\n",
    "\n",
    "We can see underdog relative games (Underdog_Rel_G) has a staggering lead over the remaining features - almost 25% of predictive power can be attributed to that one feature alone! This is sensible because it highlights how March Madness matchup favorites play more games together than their underdog opponents, which gives them more experience. This strong predictive power may negatively impact our predictions for this 2020-21 season because the COVID-19 pandemic messed up the schedules of many traditionally top-tier teams this year.\n",
    "\n",
    "The remaining dominant features are equally sensible: strength of schedule (SOS; measures the caliber of their regular season opponents), win-loss % (W-L%; what was their regular season record), and point differential (PtsDiff; what was their average margin of victory/defeat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fit data to a Random Forest to find feature importances\n",
    "rf = RandomForestClassifier().fit(prep_X_train, y_train)\n",
    "\n",
    "# Sort features & their corresponding values in by importance\n",
    "importances = rf.feature_importances_\n",
    "feat_importances = prep_all_X.columns[np.argsort(importances)]\n",
    "feat_values = np.sort(importances)\n",
    "\n",
    "# Plot findings\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.barh(feat_importances, feat_values)\n",
    "\n",
    "format_plot(title='Feature Importances', xlabel='Normalized Information Gain', ylabel='Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the Correlations Between Features?\n",
    "\n",
    "Though correlation differs from information gain, we still see the same features dominating the landscape. Even more interesting is that they're dominating to approximately the same degree that they were in our information gain plot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature matrix's correlations to target variable, then sort by absolute value\n",
    "prep_X_y = prep_X_train.merge(y_train, left_index=True, right_index=True)\n",
    "abs_desc_corr = np.abs(prep_X_y.corr().loc['Underdog_Upset']).sort_values()\n",
    "abs_desc_corr.drop('Underdog_Upset', inplace=True)\n",
    "\n",
    "# Plot findings\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.barh(abs_desc_corr.index, abs_desc_corr.values)\n",
    "\n",
    "format_plot(title='Target Variable Correlation', xlabel='Absolute Correlation Value', ylabel='Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "\n",
    "1,814 total records in a dataset isn't much, so it was necessary to employ cross-validation (CV) on the training dataset to simulate the presence of a validation set. I saw from a quick online search that a 60%-20%-20% training-validation-test set split was ideal, which required me to perform 4 CVs in my grid and randomized searches.\n",
    "\n",
    "*(Insert commentary about final model choice here)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Perform CV on chosen models with the training set, then assess their respective performances\n",
    "cv_models = get_cv_models(y)\n",
    "model_performance = evaluate_cv_models(cv_models, prep_X_train, y_train)\n",
    "     \n",
    "model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the performance of each model against the Accuracy metric\n",
    "model_performance['Best_Mean_Accuracy'].plot(figsize=(9, 6), kind='barh', xticks=np.arange(0, 1.01, 0.05))\n",
    "\n",
    "format_plot(title='Model Performance', xlabel='Metric Value', ylabel='Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identify best model to use\n",
    "best_model = cv_models['Random Forest'][-1].best_estimator_\n",
    "best_params = cv_models['Random Forest'][-1].best_params_\n",
    "\n",
    "display(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "The distribution of the predicted upsets across the tournament rounds in our test set is very similar to the true target variable values we observed in our original dataset, with the primary differences being fewer first round upsets and more third round (Sweet 16) upsets than expected. The upsets by seed pairing also shows strong similarities to our original dataset's trends.\n",
    "\n",
    "The confusion matrix at the end of this section allows us to see how this model performs in relation to precision (false positives; type I error) and recall (false negatives; type II error). Interestingly, the only metric that appears to be subpar is the recall for predicting an upset (when Underdog_Upset == 1). This suggests our model could be a little conversative and not pick upsets when it actually should, so that's important to keep in mind as we transition into making predictions for this year's March Madness matchups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all visualization data into a single DataFrame\n",
    "test_game_data = pd.concat([y_test, all_rounds, mm_matchups_df[['Seed_Favorite', 'Seed_Underdog']]], \n",
    "                           join='inner', axis=1).drop('Underdog_Upset', axis=1)\n",
    "\n",
    "# Overwrite the actual target variable (used for the join) with the best model's predictions\n",
    "y_preds = model_predictions(best_model, prep_X_test)\n",
    "test_game_data['Underdog_Upset'] = y_preds\n",
    "\n",
    "# Get seed pairs data\n",
    "test_seed_pairs = get_seed_pairs(test_game_data)\n",
    "# Extract seed pairs that resulted in upsets\n",
    "test_upset_pairs = test_seed_pairs[test_seed_pairs['Underdog_Upset'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group upset seed pairs by round and count them\n",
    "test_upset_rounds_freq = test_upset_pairs['Round'].value_counts(normalize=True)\n",
    "\n",
    "# Plot findings\n",
    "test_upset_rounds_freq.plot(figsize=(9, 6), kind='bar', rot=15)\n",
    "\n",
    "format_plot(title='Test Set Upsets by Round', xlabel='Round', ylabel='Ratio of Upsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group upsets by seed pairing and count them\n",
    "test_upset_pairs_freq = np.round(test_upset_pairs['Pairs'].value_counts(normalize=True)[:25], 3)\n",
    "\n",
    "# Plot findings\n",
    "test_upset_pairs_freq.plot(figsize=(9, 6), kind='bar', rot=35)\n",
    "\n",
    "format_plot(title='Test Set Upsets by Seed Pair', xlabel='Seed Pair', ylabel='Ratio of Upsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confusion matrix for model's test set predictions\n",
    "test_set_report = get_classification_report(y_test, y_preds)\n",
    "print(\"Test Set Metrics Report \\n\\n\", test_set_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2022 March Madness Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline\n",
    "\n",
    "We first scrape ESPN for the starting matchups in the play-in and the first round. We then automate the prediction of all rounds in the March Madness bracket. In short, the winners predicted from the first round (filled with predicted play-in winners) are reformatted to create matchups for the second round. Those second round winners are then reformatted into the third round, and so on and so forth until a champion is crowned.\n",
    "\n",
    "Should you be curious about the more intricate details, you may refer to the bracket_pipeline() custom API documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if the starting bracket CSV exists, if it doesn't then create it\n",
    "try:\n",
    "    curr_bracket_df = pd.read_csv(f'{curr_year}_march_madness_curr_start_bracket.csv')\n",
    "except FileNotFoundError:\n",
    "    curr_bracket_df = fetch.get_current_bracket('http://www.espn.com/mens-college-basketball/bracket')\n",
    "    curr_bracket_df.to_csv(f'{curr_year}_march_madness_curr_start_bracket.csv', index=False)\n",
    "\n",
    "# Extract play-in matchups\n",
    "play_in = curr_bracket_df[:4]\n",
    "play_in = play_in.reindex([0, 3, 1, 2]) # REDO REINDEXING\n",
    "\n",
    "# Extract first round matchups\n",
    "first_round = curr_bracket_df[4:]\n",
    "first_round.index = range(len(first_round))\n",
    "\n",
    "display(play_in), display(first_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create DataFrame with all March Madness predictions\n",
    "bracket_preds = bracket_pipeline(curr_year, play_in, first_round, best_model, data_cuts, null_drops)\n",
    "\n",
    "# Display bracket_pipeline() predictions by round\n",
    "for _round in bracket_preds['Round'].unique():\n",
    "    display(bracket_preds[bracket_preds['Round'] == _round])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Tournament Model Evaluation (3/17/22)\n",
    "\n",
    "The LogReg model predicted 25 upsets for the upcoming tournament, which is on the high end of the expected upsets distribution per our historical dataset EDA. This aggressiveness in upset prediction can be seen in how just as many upsets are predicted in the second round as in the first round, though based on historical trends a substantial dropoff in upsets with each successive round would be expected. This may result in the LogReg model producing a high number of false positives (resulting in low precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get seed pairs data\n",
    "curr_seed_pairs = get_seed_pairs(bracket_preds)\n",
    "# Extract seed pairs that resulted in upsets\n",
    "curr_upset_pairs = curr_seed_pairs[curr_seed_pairs['Underdog_Upset'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group upset seed pairs by round and count them\n",
    "curr_upset_rounds_freq = curr_upset_pairs['Round'].value_counts(normalize=True)\n",
    "\n",
    "# Plot findings\n",
    "curr_upset_rounds_freq.plot(figsize=(9, 6), kind='bar', rot=0)\n",
    "\n",
    "format_plot(title='Current Upsets by Round', xlabel='Round', ylabel='Ratio of Upsets')\n",
    "print(f\"Total Upsets: {len(curr_upset_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group upsets by seed pairing and count them\n",
    "curr_upset_pairs_freq = curr_upset_pairs['Pairs'].value_counts()\n",
    "\n",
    "# Plot findings\n",
    "curr_upset_pairs_freq.plot(figsize=(9, 6), kind='bar', rot=35)\n",
    "\n",
    "format_plot(title='Current Upsets by Seed Pair', xlabel='Seed Pair', ylabel='Ratio of Upsets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Tournament Model Evaluation (9/22/22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Tournament Outcomes EDA\n",
    "\n",
    "It's incredibly pleasing to see the results of the true tournament outcomes given how closely they mirrored the observed patterns in the historical dataset EDA. Not only did the tournament have exactly 20 upsets, but the distributions of said upsets across the tournament rounds and seed pairings was comparable to what was observed in both the historical and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "basic_stats_df = fetch.get_team_data(url=f\"https://www.sports-reference.com/cbb/seasons/men/{curr_year}-school-stats.html\",\n",
    "                                     attrs={'id': 'basic_school_stats'})\n",
    "clean_basic_stats_df = clean_basic_stats(basic_stats_df)\n",
    "\n",
    "# Clean basic stats' school names to properly merge with raw tournament data\n",
    "if clean_basic_stats_df['School'].str.contains('NCAA').any():\n",
    "    clean_basic_stats_df['School'] = clean_basic_stats_df['School'].apply(lambda school: school[:-5])\n",
    "\n",
    "clean_basic_stats_df['School'].replace(season_team_to_coach_team_dict, inplace=True)\n",
    "\n",
    "# Create DataFrame with all March Madness outcomes\n",
    "bracket_trues = clean_tourney_data(hist_bracket_df, clean_basic_stats_df)\n",
    "create_bracket_winners(bracket_trues)\n",
    "\n",
    "# Display true tournament results by round\n",
    "for _round in bracket_trues['Round'].unique():\n",
    "    display(bracket_trues[bracket_trues['Round'] == _round])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get seed pairs data\n",
    "true_seed_pairs = get_seed_pairs(bracket_trues)\n",
    "# Extract seed pairs that resulted in upsets\n",
    "true_upset_pairs = true_seed_pairs[true_seed_pairs['Underdog_Upset'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Group upset seed pairs by round and count them\n",
    "true_upset_rounds_freq = true_upset_pairs['Round'].value_counts(normalize=True)\n",
    "\n",
    "# Plot findings\n",
    "true_upset_rounds_freq.plot(figsize=(9, 6), kind='bar', rot=0)\n",
    "\n",
    "format_plot(title='True Upsets by Round', xlabel='Round', ylabel='Ratio of Upsets')\n",
    "print(f\"Total Upsets: {len(true_upset_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group upsets by seed pairing and count them\n",
    "true_upset_pairs_freq = true_upset_pairs['Pairs'].value_counts()\n",
    "\n",
    "# Plot findings\n",
    "true_upset_pairs_freq.plot(figsize=(9, 6), kind='bar', rot=35)\n",
    "\n",
    "format_plot(title='True Upsets by Seed Pair', xlabel='Seed Pair', ylabel='Ratio of Upsets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Predicitons to True Outcomes\n",
    "\n",
    "The LogReg model correctly predicted 40 of the 67 possible games, resulting in an accuracy of ~59.7%. Once again, the confusion matrix metrics for predicting an upset are inferior in comparison to predicting favorites' victories. However, in this case, it's the precision that bottoms out as the worst metric. This was expected given the aggressive predictive behavior observed in our LogReg pre-tournament model predictions (25 upsets predicted vs. 20 true upsets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder the tournament regions of the true outcomes to align with the predicted outcomes\n",
    "# (Necessary only when predicted and true data are scraped from different sources)\n",
    "ordered_bracket_trues = pd.concat([\n",
    "    bracket_trues.iloc[[1, 0, 2, 3]],                                                                            # Play-In\n",
    "    bracket_trues.iloc[49:57], bracket_trues.iloc[4:12], bracket_trues.iloc[34:42], bracket_trues.iloc[19:27],   # First Round\n",
    "    bracket_trues.iloc[57:61], bracket_trues.iloc[12:16], bracket_trues.iloc[42:46], bracket_trues.iloc[27:31],  # Second Round\n",
    "    bracket_trues.iloc[61:63], bracket_trues.iloc[16:18], bracket_trues.iloc[46:48], bracket_trues.iloc[31:33],  # Sweet 16\n",
    "    bracket_trues.iloc[[63, 18, 48, 33]],                                                                        # Elite 8\n",
    "    bracket_trues.iloc[[64, 65]],                                                                                # Final 4\n",
    "    bracket_trues.iloc[[66]]                                                                                     # Championship\n",
    "], ignore_index=True)\n",
    "    \n",
    "\n",
    "# Create a DataFrame that combines the tournament data predictions and true values\n",
    "trues_preds = pd.DataFrame(data={\n",
    "    'Round': ordered_bracket_trues['Round'],\n",
    "    'Predicted Winner': bracket_preds['Winner'],\n",
    "    'True Winner': ordered_bracket_trues['Winner'].replace(curr_season_to_tourney_dict)\n",
    "})\n",
    "\n",
    "display(trues_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame with the model's correct predictions\n",
    "correct_preds = trues_preds[trues_preds['True Winner'] == trues_preds['Predicted Winner']]\n",
    "\n",
    "# Calculate and output the model's accuracy\n",
    "print(f\"Accuracy: {np.round(len(correct_preds) / len(trues_preds), 3)}\")\n",
    "\n",
    "correct_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion & Future Work\n",
    "\n",
    "My machine learning approach (59.7% accuracy) isn't refined enough yet to outperform random guessing (68.7% accuracy), but I have some feature engineering and modeling ideas for improving performance:\n",
    "- #### Conference Features\n",
    " - March Madness has tradtionally been dominated by a few powerhouse teams generally found in the ACC and Big Ten conferences. I believe allowing future models to learn this trend, as well as teams' records within these conferences, will improve model performance by offsetting the dominance of the Underdog_Rel_G feature.\n",
    "- #### LogReg Probability Estimation\n",
    " - I chose the LogReg model over the SVM model during model evaluation because I believed its capability to produce probability estimations would be incredibly useful in tuning our model to avoid being overly aggressive (high false positive rate, low precision) and overly cautious (high false negative rate, low recall). I still believe this to be true, but I simply ran out of time to implement it prior to the tournament's start this year.\n",
    "- #### Additional Model Evaluation\n",
    " - Though scikit-learn models can be effective, it appears the industry is moving towards the use of gradient-boosted trees and deep learning neural nets. Implementing these models using libraries such as XGBoost and tensorflow may aid in future performance.\n",
    " \n",
    "Overall, I'm incredibly encouraged by the results of my first-ever March Madness Predictions machine learning project. The LogReg model performed decently despite COVID-19 disrupting the incredibly predictive trend discovered in the underdog relative games (Underdog_Rel_G) feature. As was mentioned above, many of the tournament's traditional powerhouses played significantly less games than normal this past season, which turned the value of Underdog_Rel_G's information gain agaisnt the model in its predictions. Hopefully this year is simply an outlier experience that can bolster the EDA and decision making of the project's future iterations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3a516bbb89c0f1ec2a087ae4624436e9d7d859fdc4249473a2895f4060f38aba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
