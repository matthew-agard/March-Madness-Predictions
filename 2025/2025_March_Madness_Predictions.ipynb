{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Python libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "# TODO: Add sklearnex to improve performance\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Global variables\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime\n",
    "start_year = 1993\n",
    "curr_year = datetime.now().year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom API\n",
    "from sys import path\n",
    "\n",
    "path.append('../API/eda')\n",
    "from data_visualizations import get_yearly_base_rates, get_seed_pairs, format_plot\n",
    "\n",
    "path.append('../API/fetch')\n",
    "import data_fetch as fetch\n",
    "\n",
    "path.append('../API/model')\n",
    "from model_selection import get_cv_models\n",
    "from model_evaluation import evaluate_cv_models, model_predictions, get_classification_report\n",
    "\n",
    "path.append('../API/preprocess')\n",
    "from feature_engineering import create_bracket_winners\n",
    "from data_integrity import season_team_to_coach_tourney_team_dict\n",
    "from data_clean import clean_basic_stats, clean_tourney_data\n",
    "from data_pipeline import dataset_pipeline, feature_pipeline, bracket_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Fetching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceived Predictors\n",
    "\n",
    "Naturally, it will be vitally important to scrape available data that is pertinent to deciding the outcome of an NCAA March Madness game between any two given teams. To successfully do so, we must break down what are generally the most influential elements of a basketball team's success.\n",
    "\n",
    "<br>Overall team performance during the regular season is generally a good indicator of how a team will perform in March Madness. This would be captured by statistics, both basic and advanced, such as the following:\n",
    "<br>**Season Record (%)\n",
    "<br>Conference Record (%); could be important given that the tournament is split into regions\n",
    "<br>Regular Season Record vs. Tourney Opponent (%); set to theoretical discrete probability of 50% if no such matchups exist \n",
    "<br>Strength of Schedule (SOS); measures the difficulty of the teams played (higher number = greater difficulty)\n",
    "<br>Top 25 Ranking (boolean); considered a consensus top-tier team\n",
    "<br>Shots Made per Game (FG, 3P, FT)\n",
    "<br>Point Differential per Game; measures how dominant/unsuccessful you are at outscoring your opponent on average\n",
    "<br>Misc. Team Stats per Game (Rebounds, Assists, Blocks, etc.)**\n",
    "\n",
    "<br>It's important to note that in the NCAA, more so than the NBA, experienced coaches can have just as much of an impact on a game's outcome as the players themselves. Hence, it's reasonable to assume that the following statistics could also be solid indicators:\n",
    "**<br>Coach March Madness Appearances\n",
    "<br>Coach Sweet Sixteen Appearances\n",
    "<br>Coach Final Four Appearances\n",
    "<br>Coach Championships Won**\n",
    "\n",
    "<br>And lastly, we need the data for the structure of the tournaments themselves:\n",
    "**<br>Favorite Seed\n",
    "<br>Underdog Seed\n",
    "<br>Round Number (0-6)\n",
    "<br>Game Outcome (boolean); did the underdog upset the favorite?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous years permitted the provision of examples as to how the predictors listed above were scraped. However, due to request rate limit restrictions from SportsReference, the inclusion of these examples in the research notebook has been discontinued."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline\n",
    "\n",
    "Below is the output of a merge of the datasets displayed above (after they've been cleaned). Once we remove the features with nulls that won't be imputed, we can begin our exploratory data analysis (EDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tournament Games Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if the historical data CSV exists, if it doesn't then create it\n",
    "prev_year = curr_year-1\n",
    "\n",
    "try:\n",
    "    mm_matchups_df = pd.read_csv(f'{curr_year}_march_madness_hist_data.csv')\n",
    "except FileNotFoundError:\n",
    "    hist_matchups_df = pd.read_csv(f'../{prev_year}/{prev_year}_march_madness_hist_data.csv')\n",
    "    curr_matchups_df = dataset_pipeline([prev_year])\n",
    "    \n",
    "    mm_matchups_df = pd.concat([hist_matchups_df, curr_matchups_df])\n",
    "    mm_matchups_df.to_csv(f'{curr_year}_march_madness_hist_data.csv', index=False)\n",
    "\n",
    "mm_matchups_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Nulls\n",
    "\n",
    "Recall that our cleaned dataset has a total of 1,877 March Madness games present, and each March Madness will consist of no more than 67 games (4 play-in games + 63 tournament games). As can be seen below, the advanced stats pertaining to personal fouls (PF) and turnovers (TOV) are the only features listed with no more than a season's worth of missing data. Given how rapidly the gameplay of college basketball has evolved over the time horizon of our dataset, imputing nulls on multiple years of March Madness data would likely be a futile effort. The features with a high volume of nulls (1> season) will be dropped from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Given that a feature has any nulls, find the number of nulls present\n",
    "true_nulls = fetch.get_feature_null_counts(mm_matchups_df)\n",
    "true_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get turnover features from true_nulls\n",
    "tov_null_fills = [col for col in true_nulls.index if ('TOV' in col)]\n",
    "\n",
    "# Get personal foul features from true_nulls\n",
    "pf_null_fills = [col for col in true_nulls.index if ('PF' in col)]\n",
    "\n",
    "# All other features found in true_nulls are dropped from our original dataset\n",
    "null_drops = list(set(true_nulls.index) - set(tov_null_fills) - set(pf_null_fills))\n",
    "mm_matchups_df.drop(null_drops, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the rows containing all of the dataset's nulls to be imputed\n",
    "# This will be a useful reference to validate the proper imputation of the nulls\n",
    "tov_nulls_rows = fetch.get_null_rows(tov_null_fills, mm_matchups_df)\n",
    "pf_nulls_rows = fetch.get_null_rows(pf_null_fills, mm_matchups_df)\n",
    "\n",
    "display(tov_nulls_rows), display(pf_nulls_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the Distributions of the Features We Wish to Impute?\n",
    "\n",
    "Upon looking at the distributions below of all turnover and personal foul features, we can see that they closely resemble a normal distribution. Given this assumption, the features' means will serve as good values for imputing the nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the distributions of the dataset's turnover features\n",
    "tov_null_years = sorted(list(set(tov_nulls_rows['Year'])))\n",
    "\n",
    "for year in tov_null_years:\n",
    "    print(f\"{year} feature distributions\")\n",
    "    year_df = mm_matchups_df[mm_matchups_df['Year'] == year]\n",
    "    year_df[tov_null_fills].hist(figsize=(10, 5), layout=(len(tov_null_years), len(tov_null_fills)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the distributions of the dataset's personal foul features\n",
    "pf_null_years = sorted(list(set(pf_nulls_rows['Year'])))\n",
    "\n",
    "mm_matchups_df[pf_null_fills].hist(figsize=(10, 5), layout=(1, len(pf_null_fills)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute TOV Nulls by Season & Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve TOV feature means by season; only include seasons that contain nulls\n",
    "tov_col_means = mm_matchups_df[tov_nulls_rows.columns].groupby(['Year', 'Underdog_Upset']).mean()\n",
    "tov_col_means.loc[tov_null_years]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in tov_null_years:\n",
    "    for label in [0, 1]:\n",
    "        for col in tov_null_fills:\n",
    "            # Get feature's rows with nulls for given year & label type\n",
    "            fill_condition = (tov_nulls_rows['Year'] == year) & (tov_nulls_rows['Underdog_Upset'] == label)\n",
    "            col_fill_rows = tov_nulls_rows[fill_condition].index\n",
    "            # Get feature's mean for given year\n",
    "            col_year_mean = np.round(tov_col_means.loc[(year, label), col], 1)\n",
    "            # Impute nulls of interest\n",
    "            mm_matchups_df.loc[col_fill_rows, col] = mm_matchups_df.loc[col_fill_rows, col].fillna(col_year_mean)\n",
    "\n",
    "# Display rows that originally had nulls to see if they match discovered feature means\n",
    "mm_matchups_df.loc[tov_nulls_rows.index, tov_null_fills]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute PF Nulls by Target Variable Using Entire Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve PF feature means; only found in one season\n",
    "pf_col_means = mm_matchups_df[pf_nulls_rows.columns].groupby(['Underdog_Upset']).mean()\n",
    "pf_col_means.drop('Year', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for label in [0, 1]:\n",
    "    for col in pf_null_fills:\n",
    "        # Get feature's rows with nulls for given year & label type\n",
    "        fill_condition = (pf_nulls_rows['Underdog_Upset'] == label)\n",
    "        col_fill_rows = pf_nulls_rows[fill_condition].index\n",
    "        # Get feature's mean\n",
    "        col_mean = np.round(pf_col_means.loc[label, col], 1)\n",
    "        # Impute nulls of interest\n",
    "        mm_matchups_df.loc[col_fill_rows, col] = mm_matchups_df.loc[col_fill_rows, col].fillna(col_mean)\n",
    "    \n",
    "# Display rows that originally had nulls to see if they match discovered feature means\n",
    "mm_matchups_df.loc[pf_nulls_rows.index, pf_null_fills]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "\n",
    "As any good data scientist should do, there are a few questions I hope to address in my EDA:\n",
    " - What is our bracket's accuracy if we guess the favorite always wins?\n",
    " - How often do upsets occur in a given year's March Madness?\n",
    " - What is the distribution of upsets across the tournament rounds?\n",
    " - Which seeding combinations are most likely to produce upsets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is our Bracket's Accuracy if We Guess the Favorite Always Wins?\n",
    "\n",
    "This is a strategy many of us have employed at least once while filling out a March Madness bracket, myself included. This is a classic EDA question which explores the idea of the base rate: historically, how often would we be right if we always assumed the favorite won the March Madness matchup? ~68.7% of our predictions would be correct according to our data, approximately 2 out of every 3 games. This means that for any model to be of value to us, it must demonstrate >68.7% of its predictions are correct.\n",
    "\n",
    "In college basketball, the best (and thus most impactful) players typically leave the NCAA after no more than 2 years. Thus, a 2-year moving average was chosen to illustrate the base rate's trends as new impactful players participate in March Madness. Our base rate is maintained fairly consistently over time, with a few exceptions scattered across the dataset's time horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each season's base rate\n",
    "yearly_base_rates = get_yearly_base_rates(mm_matchups_df)\n",
    "# Dataset's mean base rate\n",
    "mean_base_rate = np.round(yearly_base_rates.mean(), 3)\n",
    "# Moving average\n",
    "years_ma = 2\n",
    "base_rate_ma = np.round(yearly_base_rates.rolling(years_ma).mean(), 3)\n",
    "\n",
    "# Plot findings\n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "plt.plot(yearly_base_rates.index, [mean_base_rate] * len(yearly_base_rates), color='k', linewidth=3, label=f'Mean ({mean_base_rate})')\n",
    "plt.plot(yearly_base_rates.index, base_rate_ma, color='r', linewidth=3, label=f'{years_ma}-Yr MA')\n",
    "plt.bar(yearly_base_rates.index, yearly_base_rates)\n",
    "\n",
    "format_plot(title='Dataset Base Rate Trends', xlabel='Season', ylabel='Base Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Often Do Upsets Occur in a Given Year's March Madness?\n",
    "\n",
    "If favorites win March Madness matchups ~68.7% of the time, then we can conclude the underdogs are winning the other 31.3%; this translates to approximately 20 upsets per year. This knowledge gives us insight into how many upsets we should expect our model to predict (i.e. 25 is likely too high, 15 is likely too low).\n",
    "\n",
    "A 2-year moving average has been employed once again to highlight the trends present across the dataset's time horizon. At first it may appear to be more volatile than the base rate trends, but this is simply due to the scale of the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each season's upset count\n",
    "yearly_upsets = mm_matchups_df.groupby('Year').agg({'Underdog_Upset': 'sum'})\n",
    "# Dataset's mean upset count\n",
    "mean_upsets = np.round(yearly_upsets['Underdog_Upset'].mean(), 1)\n",
    "# Moving average\n",
    "upsets_ma = np.round(yearly_upsets.rolling(years_ma).mean(), 1)\n",
    "\n",
    "# Plot findings\n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "plt.plot(yearly_upsets.index, [mean_upsets] * len(yearly_upsets), color='k', linewidth=3, label=f'Mean ({mean_upsets})')\n",
    "plt.plot(yearly_upsets.index, upsets_ma, color='r', linewidth=3, label=f'{years_ma}-Yr MA')\n",
    "plt.bar(yearly_upsets.index, yearly_upsets['Underdog_Upset'])\n",
    "\n",
    "format_plot(title='Dataset Upsets Volume Trends', xlabel='Season', ylabel='Upsets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the Distribution of Upsets Across the Tournament Rounds?\n",
    "\n",
    "Of the 20 upsets that we can expect in any given March Madness, the visualization below gives us an idea of how many upsets we can expect in each round. Nearly 70% of all upsets happen in the first 2 rounds, which is sensible given that 75% of all games are held in the first 2 rounds. Fewer upsets in each successive round is expected not only because of a diminished volume of games, but also a greater saturation of top-tier teams amongst the remaining teams in contention. We should expect our model to follow a similar pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve data about March Madness matchup seed pairings\n",
    "seed_pairs = get_seed_pairs(mm_matchups_df)\n",
    "# Extract seed pairs that resulted in upsets\n",
    "upset_pairs = seed_pairs[seed_pairs['Underdog_Upset'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group upset seed pairs by round and count them\n",
    "upset_rounds_freq = upset_pairs['Round'].value_counts(normalize=True)\n",
    "\n",
    "# Plot findings\n",
    "upset_rounds_freq.plot(figsize=(9, 6), kind='bar', rot=15)\n",
    "\n",
    "format_plot(title='Dataset Upsets by Round', xlabel='Round', ylabel='Ratio of Upsets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which Seeding Combinations are the Most Likely to Produce Upsets?\n",
    "\n",
    "Suppose our model predicts 12 upsets in the first round, even though we should only expect 9 according to our EDA. In the event this type of situation arises, it'd be valuable to know which seed pairings are most likely to produce upsets so we could discard 3 of the 12 first round upsets with the lowest upset likelihoods.\n",
    "\n",
    "Only the top 25 upset likelihoods are shown because our EDA also revealed we should expect no more than 25 upsets in a given year. We can see once again that the overwhelming majority of upsets occur in the first round, particularly in the 4 seed pairings that are the most evenly matched (Seeds 8 vs. 9 through Seeds 5 vs. 12). Our model should emulate this behavior fairly closely in its upset predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group upsets by seed pairing and count them\n",
    "upset_pairs_freq = np.round(upset_pairs['Pairs'].value_counts(normalize=True)[:25], 3)\n",
    "\n",
    "# Plot findings\n",
    "upset_pairs_freq.plot(figsize=(9, 6), kind='bar', rot=35)\n",
    "\n",
    "format_plot(title='Dataset Upsets by Seed Pair', xlabel='Seed Pair', ylabel='Ratio of Upsets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering & Analysis\n",
    "\n",
    "Below we can see our original feature matrix (X) and then our feature matrix prepped for model fitting (prep_all_X). This transformation is performed by passing X through the feature pipeline. \n",
    "\n",
    "One of the primary actions that transpires in the pipeline is subtracting the favorites' stats from the underdogs' stats in each matchups to create underdog-relative features. This not only retains virtually all of our potential information gain, but also improves computation speed and reduces the likelihood of high variance and thus overfitting the data.\n",
    "\n",
    "The second primary action is the scaling of our numerical features. This conversion of all numerical features to a normal distribution is important because it eliminates the possibility of features' number ranges negatively influencing a model's learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop unneeded features\n",
    "mm_matchups_df.drop(['Year', 'Team_Favorite', 'Team_Underdog'], axis=1, inplace=True)\n",
    "# Store rounds data (for EDA visualizations)\n",
    "all_rounds = mm_matchups_df['Round']\n",
    "\n",
    "# Create feature matrix and target variable\n",
    "X = mm_matchups_df.drop('Underdog_Upset', axis=1)\n",
    "y = mm_matchups_df['Underdog_Upset']\n",
    "\n",
    "display(X), display(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"We'll stratify the 80-20 split of our training & test datasets according to the target\n",
    "variable's distribution so our model can learn the trends observed in our EDA\"\"\"\n",
    "# Drop round data because it's already stored in previous notebook cell\n",
    "X.drop('Round', axis=1, inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, \n",
    "                                                    stratify=pd.concat([y, all_rounds], axis=1))\n",
    "\n",
    "# All datasets will be scaled based on fit found for training dataset\n",
    "basic_stats_df = fetch.get_team_data(url=f\"https://www.sports-reference.com/cbb/seasons/men/{year}-school-stats.html\",\n",
    "                                     attrs={'id': 'basic_school_stats'})\n",
    "basic_stats_cols = clean_basic_stats(basic_stats_df).columns\n",
    "data_cuts = {    \n",
    "    'FULL': X,\n",
    "    'TRAIN': X_train,\n",
    "    'TEST': X_test,\n",
    "}\n",
    "\n",
    "# Pass datasets through feature pipeline to prep them for model fitting\n",
    "prep_X_train = feature_pipeline('TRAIN', data_cuts, basic_stats_cols)\n",
    "prep_X_test = feature_pipeline('TEST', data_cuts, basic_stats_cols)\n",
    "\n",
    "prep_all_X = pd.concat([prep_X_train, prep_X_test])\n",
    "prep_all_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the Distributions of all our Features?\n",
    "\n",
    "The illustration below shows us the impact of the feature scaling mentioned earlier. Our features are predominantly normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_all_X.hist(figsize=(15, 10))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Features have the Greatest Predictive Power?\n",
    "The plot below represents an important concept in data science & machine learning: normalized information gain. It's a ratio of how much predictive power can be attributed to each feature in a feature matrix; these values should sum to 1, or 100% of the predictive power.\n",
    "\n",
    "We can see underdog relative games (Underdog_Rel_G) has a staggering lead over the remaining features - 15+% of predictive power can be attributed to that one feature alone! This is sensible because it highlights how March Madness matchup favorites play more games together than their underdog opponents, which gives them more experience. This strong predictive power likely impacted our predictions in a negative way for the inaugural 2020-21 tourney because of the COVID-19 pandemic's distortion of team schedules, but that should no longer be a factor. The remaining dominant features are equally sensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit data to a Random Forest to find feature importances\n",
    "rf = RandomForestClassifier().fit(prep_X_train, y_train)\n",
    "\n",
    "# Sort features & their corresponding values in by importance\n",
    "importances = rf.feature_importances_\n",
    "feat_importances = prep_all_X.columns[np.argsort(importances)]\n",
    "feat_values = np.sort(importances)\n",
    "\n",
    "# Plot findings\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.barh(feat_importances, feat_values)\n",
    "\n",
    "format_plot(title='Feature Importances', xlabel='Normalized Information Gain', ylabel='Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the Correlations Between Features?\n",
    "\n",
    "Though correlation differs from information gain, we still see the same features dominating the landscape. Even more interesting is that they're dominating to approximately the same degree that they were in our information gain plot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature matrix's correlations to target variable, then sort by absolute value\n",
    "prep_X_y = prep_X_train.merge(y_train, left_index=True, right_index=True)\n",
    "abs_desc_corr = np.abs(prep_X_y.corr().loc['Underdog_Upset']).sort_values()\n",
    "abs_desc_corr.drop('Underdog_Upset', inplace=True)\n",
    "\n",
    "# Plot findings\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.barh(abs_desc_corr.index, abs_desc_corr.values)\n",
    "\n",
    "format_plot(title='Target Variable Correlation', xlabel='Absolute Correlation Value', ylabel='Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "\n",
    "1,877 total records in a dataset isn't much, so it was necessary to employ cross-validation (CV) on the training dataset to simulate the presence of a validation set. I saw from a quick online search that a 60%-20%-20% training-validation-test set split was ideal, which required me to perform 4 CVs in my grid and randomized searches.\n",
    "\n",
    "The models were evaluated based solely upon both the Accuracy metric, with the Random Forest (RF) and XGBoost models as the frontrunners. All factors considered, I believe the RF model is the appropriate model selection given its reduced overfitting likelihood compared to XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Perform CV on chosen models with the training set, then assess their respective performances\n",
    "cv_models = get_cv_models(y)\n",
    "model_performance = evaluate_cv_models(cv_models, prep_X_train, y_train)\n",
    "     \n",
    "model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the performance of each model against the Accuracy metric\n",
    "model_performance['Best_Mean_Accuracy'].plot(figsize=(9, 6), kind='barh', xticks=np.arange(0, 1.01, 0.05))\n",
    "\n",
    "format_plot(title='Model Performance', xlabel='Metric Value', ylabel='Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best model to use\n",
    "selected_model = 'Random Forest'\n",
    "\n",
    "best_model = cv_models[selected_model][-1].best_estimator_\n",
    "best_model.fit(prep_X_train, y_train)\n",
    "\n",
    "best_params = cv_models[selected_model][-1].best_params_\n",
    "display(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "The distribution of the predicted upsets across the tournament rounds in our test set is very similar to the true target variable values we observed in our original dataset. The upsets by seed pairing also shows strong similarities to our original dataset's trends.\n",
    "\n",
    "The confusion matrix at the end of this section allows us to see how this model performs in relation to precision (false positives; type I error) and recall (false negatives; type II error). Interestingly, the recall for predicting an upset (Underdog_Upset == 1) is once again the only underperforming metric. This suggests our model could be a little conversative and not pick upsets when it actually should, so that's important to keep in mind as we transition into making predictions for this year's March Madness matchups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all visualization data into a single DataFrame\n",
    "test_game_data = pd.concat([y_test, all_rounds, mm_matchups_df[['Seed_Favorite', 'Seed_Underdog']]], \n",
    "                           join='inner', axis=1).drop('Underdog_Upset', axis=1)\n",
    "\n",
    "# Overwrite the actual target variable (used for the join) with the best model's predictions\n",
    "y_preds = model_predictions(best_model, prep_X_test)\n",
    "test_game_data['Underdog_Upset'] = y_preds\n",
    "\n",
    "# Get seed pairs data\n",
    "test_seed_pairs = get_seed_pairs(test_game_data)\n",
    "# Extract seed pairs that resulted in upsets\n",
    "test_upset_pairs = test_seed_pairs[test_seed_pairs['Underdog_Upset'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group upset seed pairs by round and count them\n",
    "test_upset_rounds_freq = test_upset_pairs['Round'].value_counts(normalize=True)\n",
    "\n",
    "# Plot findings\n",
    "test_upset_rounds_freq.plot(figsize=(9, 6), kind='bar', rot=15)\n",
    "\n",
    "format_plot(title='Test Set Upsets by Round', xlabel='Round', ylabel='Ratio of Upsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group upsets by seed pairing and count them\n",
    "test_upset_pairs_freq = np.round(test_upset_pairs['Pairs'].value_counts(normalize=True)[:25], 3)\n",
    "\n",
    "# Plot findings\n",
    "test_upset_pairs_freq.plot(figsize=(9, 6), kind='bar', rot=35)\n",
    "\n",
    "format_plot(title='Test Set Upsets by Seed Pair', xlabel='Seed Pair', ylabel='Ratio of Upsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confusion matrix for model's test set predictions\n",
    "test_set_report = get_classification_report(y_test, y_preds)\n",
    "print(\"Test Set Metrics Report \\n\\n\", test_set_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2025 March Madness Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline\n",
    "\n",
    "We first scrape ESPN for the starting matchups in the play-in and the first round. We then automate the prediction of all rounds in the March Madness bracket. In short, the winners predicted from the first round (filled with predicted play-in winners) are reformatted to create matchups for the second round. Those second round winners are then reformatted into the third round, and so on until a champion is crowned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if the starting bracket CSV exists, if it doesn't then create it\n",
    "try:\n",
    "    curr_bracket_df = pd.read_csv(f'{curr_year}_march_madness_curr_start_bracket.csv')\n",
    "except FileNotFoundError:\n",
    "    curr_bracket_df = fetch.get_current_bracket(curr_year)\n",
    "    curr_bracket_df.to_csv(f'{curr_year}_march_madness_curr_start_bracket.csv', index=False)\n",
    "    raise ValueError(\"Manually add First Four matchups to starting bracket before resuming program execution\")\n",
    "\n",
    "# Extract play-in matchups\n",
    "play_in = curr_bracket_df[:4]\n",
    "\n",
    "# Extract first round matchups\n",
    "first_round = curr_bracket_df[4:]\n",
    "# Reorder to ensure Final Four region matchups are correct\n",
    "first_round = pd.concat([first_round.iloc[:8], first_round.iloc[24:], first_round.iloc[8:24]], axis=0)\n",
    "first_round.index = range(len(first_round))\n",
    "\n",
    "display(play_in), display(first_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create DataFrame with all March Madness predictions\n",
    "bracket_preds = bracket_pipeline(curr_year, play_in, first_round, best_model, data_cuts, null_drops)\n",
    "\n",
    "# Display bracket_pipeline() predictions by round\n",
    "for _round in bracket_preds['Round'].unique():\n",
    "    display(bracket_preds[bracket_preds['Round'] == _round])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Tournament Model Evaluation (3/20/24)\n",
    "\n",
    "The RF model predicted 22 upsets for the upcoming tournament just like last year, a few higher than the expected number of upsets per our historical dataset EDA. This may be considered moderately aggressive behavior, causing the RF model to generate poor upset prediction precision. Alternatively, this may prove to be good decision-making on the model's part given the test set confusion matrix's indication of the model's potential for poor upset prediction recall. Only time will tell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get seed pairs data\n",
    "curr_seed_pairs = get_seed_pairs(bracket_preds)\n",
    "# Extract seed pairs that resulted in upsets\n",
    "curr_upset_pairs = curr_seed_pairs[curr_seed_pairs['Underdog_Upset'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group upset seed pairs by round and count them\n",
    "curr_upset_rounds_freq = curr_upset_pairs['Round'].value_counts(normalize=True)\n",
    "\n",
    "# Plot findings\n",
    "curr_upset_rounds_freq.plot(figsize=(9, 6), kind='bar', rot=0)\n",
    "\n",
    "format_plot(title='Current Upsets by Round', xlabel='Round', ylabel='Ratio of Upsets')\n",
    "print(f\"Total Upsets: {len(curr_upset_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group upsets by seed pairing and count them\n",
    "curr_upset_pairs_freq = curr_upset_pairs['Pairs'].value_counts()\n",
    "\n",
    "# Plot findings\n",
    "curr_upset_pairs_freq.plot(figsize=(9, 6), kind='bar', rot=35)\n",
    "\n",
    "format_plot(title='Current Upsets by Seed Pair', xlabel='Seed Pair', ylabel='Ratio of Upsets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Tournament Model Evaluation (TBD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Tournament Outcomes EDA\n",
    "\n",
    "It's incredibly pleasing to see the results of the true tournament outcomes given how closely they mirrored the observed patterns in the historical dataset EDA. The tournament had 19 upsets in total, and the distributions of said upsets across the tournament rounds and seed pairings were comparable to what was observed in both the historical and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "basic_stats_df = fetch.get_team_data(url=f\"https://www.sports-reference.com/cbb/seasons/men/{curr_year}-school-stats.html\",\n",
    "                                     attrs={'id': 'basic_school_stats'})\n",
    "clean_basic_stats_df = clean_basic_stats(basic_stats_df)\n",
    "\n",
    "# Clean basic stats' school names to properly merge with raw tournament data\n",
    "if clean_basic_stats_df['School'].str.contains('NCAA').any():\n",
    "    clean_basic_stats_df['School'] = clean_basic_stats_df['School'].apply(lambda school: school[:-5])\n",
    "\n",
    "# Create DataFrame with all March Madness outcomes\n",
    "bracket_trues = clean_tourney_data(fetch.get_tourney_matchups(curr_year), clean_basic_stats_df)\n",
    "create_bracket_winners(bracket_trues)\n",
    "\n",
    "# Display true tournament results by round\n",
    "for _round in bracket_trues['Round'].unique():\n",
    "    display(bracket_trues[bracket_trues['Round'] == _round])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get seed pairs data\n",
    "true_seed_pairs = get_seed_pairs(bracket_trues)\n",
    "# Extract seed pairs that resulted in upsets\n",
    "true_upset_pairs = true_seed_pairs[true_seed_pairs['Underdog_Upset'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group upset seed pairs by round and count them\n",
    "true_upset_rounds_freq = true_upset_pairs['Round'].value_counts(normalize=True)\n",
    "\n",
    "# Plot findings\n",
    "true_upset_rounds_freq.plot(figsize=(9, 6), kind='bar', rot=0)\n",
    "\n",
    "format_plot(title='True Upsets by Round', xlabel='Round', ylabel='Ratio of Upsets')\n",
    "print(f\"Total Upsets: {len(true_upset_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group upsets by seed pairing and count them\n",
    "true_upset_pairs_freq = true_upset_pairs['Pairs'].value_counts()\n",
    "\n",
    "# Plot findings\n",
    "true_upset_pairs_freq.plot(figsize=(9, 6), kind='bar', rot=35)\n",
    "\n",
    "format_plot(title='True Upsets by Seed Pair', xlabel='Seed Pair', ylabel='Ratio of Upsets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Predicitons to True Outcomes\n",
    "\n",
    "As can be seen below, the RF model correctly predicted 50 of the 63 possible games, resulting in an accuracy of ~79.4%! Though not nearly as substantial a jump as last year's performance of ~76.2% (48/63), I acknowledge that this marginal increase is more realistic given the law of diminishing returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display correct predictions by round\n",
    "correct_preds_df = pd.DataFrame(columns=['# Correct Preds', '# Possible Preds'])\n",
    "\n",
    "for _round in bracket_trues['Round'].unique():\n",
    "    # Extract round data from predictions & trues\n",
    "    trues_round = bracket_trues[bracket_trues['Round'] == _round]\n",
    "    preds_round = bracket_preds[bracket_preds['Round'] == _round]\n",
    "    \n",
    "    # Intersection of predictions & trues indicates the model's correct picks\n",
    "    correct_preds = set(trues_round['Winner']).intersection(set(preds_round['Winner']))\n",
    "    \n",
    "    # Input data into Dataframe\n",
    "    correct_preds_df.loc[_round] = [len(correct_preds), len(preds_round)]\n",
    "    \n",
    "correct_preds_df.loc['Tournament Total'] = [correct_preds_df['# Correct Preds'].sum(), correct_preds_df['# Possible Preds'].sum()]\n",
    "    \n",
    "correct_preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate and output the model's accuracy\n",
    "print(f\"Accuracy: {np.round(correct_preds_df.loc['Tournament Total', '# Correct Preds'] / len(bracket_trues), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion & Future Work\n",
    "\n",
    "My machine learning approach (79.4% accuracy) once again outperformed the random guessing benchmark (68.7% accuracy)! My lifetime achievement goal was 80%, and at this rate it looks like I'll be able to make that happen in next year's tourney. So naturally, I still have some feature engineering and modeling ideas for improving performance:\n",
    "- #### Additional Model Evaluation\n",
    " - I successfully implemented XGBoost for use in model evaluation, a feat I'm incredibly proud of given it being a brand new domain for me. However, I was unable to implement a deep learning neural network using tensorflow, and that's still a goal of mine to potentially aid future performance.\n",
    "- #### Model Hyperparameter Tuning\n",
    " - Though the law of diminishing returns does play a factor in this year's marginal performance improvement, it may very well also be attributed to virtually no difference in the hyperparameter tuning of my models. I still need to learn how to properly tune XGBoost models and neural networks, with which I'm a novice at best. Doing so may see one of those models claiming the crown for use in future predictions, ideally with improved performance.\n",
    " \n",
    "This year's March Madness Predictions machine learning project was very affirming for me. In my mind, a second consecutive year of handily beating the random guessing benchmark solidifies the legitimacy of my research and validates all the hours I've put into this personal project. The only notable difference between this year's and last year's projects was the addition of another year of tourney data to my historical dataset. More data always helps, but I have yet to exhaust all of my options for model tuning and metric optimization to continually enhance my performance. I look forward to continuing this pursuit for next year."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
